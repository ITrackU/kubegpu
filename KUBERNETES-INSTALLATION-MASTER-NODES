# GPU-Accelerated k3s Cluster Setup Guide

## Before Installation - Cleanup (Critical)
```bash
# On all nodes (master and agents)
sudo /usr/local/bin/k3s-uninstall.sh || true
sudo /usr/local/bin/k3s-agent-uninstall.sh || true
sudo rm -rf /var/lib/rancher/k3s /etc/rancher
sudo rm -f /etc/containerd/config.toml
sudo systemctl daemon-reload
```

## 1. OS Preparation (All Nodes)
```bash
sudo dnf update -y
sudo dnf install -y curl vim git conntrack-tools socat iptables

# Disable swap
sudo swapoff -a
sudo sed -i '/ swap /d' /etc/fstab

# Kernel settings
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward = 1
EOF
sudo sysctl --system

# SELinux config
sudo setenforce 0
sudo sed -i 's/^SELINUX=.*/SELINUX=permissive/' /etc/selinux/config
```

## 2. Master Node Setup
```bash
curl -sfL https://get.k3s.io | \
  INSTALL_K3S_EXEC="--disable=servicelb --disable=traefik --node-taint CriticalAddonsOnly=true:NoExecute" \
  sh -

# Verify
sudo k3s kubectl get nodes
MASTER_IP=$(hostname -I | awk '{print $1}')
NODE_TOKEN=$(sudo cat /var/lib/rancher/k3s/server/node-token)
echo "Master IP: $MASTER_IP | Node token: $NODE_TOKEN"
```

## 3. GPU Node Setup (Per Agent)

### A. NVIDIA Stack Installation
```bash
sudo dnf install -y kernel-devel gcc make

distribution=$(. /etc/os-release; echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.repo | \
  sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo
```

### B. Containerd Configuration
```bash
sudo mkdir -p /etc/containerd
containerd config default | sed \
  -e 's|/usr/bin/containerd|/usr/bin/containerd|' \
  -e 's|"runtime_type": "io.containerd.runtime.v1.linux"|"runtime_type": "io.containerd.runc.v2"|' | \
  sudo tee /etc/containerd/config.toml

sudo tee -a /etc/containerd/config.toml <<'EOF'

[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.nvidia]
  runtime_type = "io.containerd.runc.v2"
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.nvidia.options]
    BinaryName = "/usr/bin/nvidia-container-runtime"
EOF

sudo nvidia-ctk runtime configure --runtime=containerd
sudo systemctl restart containerd
```

### C. Join Cluster
```bash
curl -sfL https://get.k3s.io | \
  K3S_URL=https://${MASTER_IP}:6443 \
  K3S_TOKEN=${NODE_TOKEN} \
  INSTALL_K3S_EXEC="--docker" \
  sh -
```

## 4. Post-Install Configuration

### A. Node Labeling/Tainting (on master)
```bash
# For each GPU node
sudo k3s kubectl label node <node-name> node.kubernetes.io/gpu=true
sudo k3s kubectl taint node <node-name> gpu=true:NoSchedule
```

### B. NVIDIA Device Plugin
```bash
sudo k3s kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.13.0/nvidia-device-plugin.yml

# Verify resources
sudo k3s kubectl get nodes -o json | jq '.items[].status.allocatable'
```

## 5. GPU Test Workloads

### Option 1: Quick Pod Test
```yaml
# test-gpu-pod.yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-gpu-pod
spec:
  restartPolicy: OnFailure
  containers:
  - name: cuda-test
    image: nvidia/cuda:11.0-base
    command: ["nvidia-smi"]
    resources:
      limits:
        nvidia.com/gpu: 1
  tolerations:
  - key: "gpu"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"
```

### Option 2: Job-based Test
```yaml
# gpu-test-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: cuda-smi
spec:
  template:
    spec:
      containers:
      - name: nvidia-smi
        image: nvidia/cuda:12.4.0-runtime-ubuntu22.04
        command: 
          - nvidia-smi
          - --query-gpu=name,driver_version,memory.total
          - --format=csv
        resources:
          limits:
            nvidia.com/gpu: 1
      restartPolicy: Never
      tolerations:
      - key: "gpu"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
  backoffLimit: 2
```

Apply and verify:
```bash
sudo k3s kubectl apply -f test-gpu-pod.yaml
sudo k3s kubectl logs test-gpu-pod

# OR
sudo k3s kubectl apply -f gpu-test-job.yaml
sudo k3s kubectl logs job/cuda-smi
```

## Troubleshooting Guide

| Symptom | Checkpoints |
|---------|-------------|
| Nodes not joining | `systemctl status k3s-agent` <br> `journalctl -u k3s-agent -n 100` |
| No GPU resources | `nvidia-smi` (on node) <br> `kubectl describe node | grep -A 10 Allocatable` <br> `kubectl logs -n kube-system -l app=nvidia-device-plugin-daemonset` |
| GPU permissions | `ls -l /dev/nvidia*` <br> `dmesg | grep -i nvidia` |
| Container runtime issues | `sudo journalctl -u containerd -n 200` |
| Network problems | `sudo k3s kubectl get nodes -o wide` <br> `ping <master-ip>` <br> `nc -vz <master-ip> 6443` |

### Troubleshooting Guide for NVIDIA Driver Mismatch

| Symptom | Checkpoints |
|---------|-------------|
| **Driver/library version mismatch** | <ul><li>`nvidia-smi`: `Failed to initialize NVML: Driver/library version mismatch`</li><li>Containers fail to start with GPU errors</li><li>`dmesg | grep -i nvidia` shows version conflicts</li></ul>**Solution:**<br>```bash<br>sudo dkms install nvidia/$(ls /usr/src \| grep nvidia \| head -n1 \| cut -d'-' -f2-) -k $(uname -r)<br>sudo modprobe -r nvidia_uvm nvidia_drm nvidia_modeset nvidia<br>sudo modprobe nvidia<br>sudo systemctl restart containerd<br>```<br>If persists: `sudo apt install --reinstall nvidia-driver-$(dpkg -l \| grep 'nvidia-driver' \| awk '{print $3}')` |

### Preventive Maintenance Commands

To avoid version mismatches after kernel/driver updates:

1. **Before kernel updates:**
```bash
# Preserve current NVIDIA driver modules
sudo dkms install --force -m nvidia -v $(modinfo -F version nvidia) -k $(uname -r)

# Create rebuild script (run after reboot)
sudo tee /usr/local/sbin/refresh-nvidia <<'EOF'
#!/bin/bash
current_kernel=$(uname -r)
sudo dkms install -m nvidia -v $(ls /usr/src | grep nvidia | head -n1 | cut -d'-' -f2-) -k $current_kernel
sudo update-initramfs -u
sudo systemctl restart containerd
EOF
sudo chmod +x /usr/local/sbin/refresh-nvidia
```

2. **After kernel updates:**
```bash
# Verify module consistency
sudo dkms status | grep nvidia

# Rebuild modules for current kernel
sudo dkms autoinstall -k $(uname -r)
```

3. **After NVIDIA driver updates:**
```bash
# Verify loaded vs. installed versions
lsmod | grep nvidia
ls -d /usr/src/nvidia-*

# Force DKMS rebuild if needed
sudo dkms remove -m nvidia -v $(ls /usr/src | grep nvidia | head -n1 | cut -d'-' -f2-) --all
sudo dkms install -m nvidia -v $(ls /usr/src | grep nvidia | head -n1 | cut -d'-' -f2-)

# Reconfigure Containerd
sudo nvidia-ctk runtime configure --runtime=containerd
sudo systemctl restart containerd
```

### Automated Maintenance Script
```bash
#!/bin/bash
# nvidia-module-sync.sh
KERNEL=$(uname -r)
DRIVER=$(ls /usr/src | grep nvidia | head -n1 | cut -d'-' -f2-)

# Rebuild for current kernel
sudo dkms install -m nvidia -v $DRIVER -k $KERNEL

# Update library links
sudo rm -f /usr/lib/x86_64-linux-gnu/nvidia/current
sudo ln -s /usr/lib/x86_64-linux-gnu/nvidia/$DRIVER /usr/lib/x86_64-linux-gnu/nvidia/current

# Verify consistency
lsmod | grep nvidia | grep $DRIVER || {
    echo "Reloading modules"
    sudo modprobe -r nvidia_uvm nvidia_drm nvidia_modeset nvidia
    sudo modprobe nvidia
    sudo systemctl restart containerd
}
```

## Key Improvements Incorporated:
1. **Mandatory cleanup** before installation
2. **Centralized NVIDIA configuration** with containerd fixes
3. **Simpler GPU test pod** with tolerations
4. **Troubleshooting table** for common issues
5. **Token/IP capture** automation in master setup
6. **Combined device plugin/verification** steps
7. **Network/host preparation** standardization
8. **Runtime validation** for containerd config

> **Note**: Replace `<node-name>`, `<master-ip>`, and `<node-token>` with your actual values. Remember to `export MASTER_IP` and `export NODE_TOKEN` on agent nodes before joining.
